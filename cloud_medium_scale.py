#!/usr/bin/env python3
"""
ä¸­ç­‰è¦æ¨¡é›²ç«¯æ¨ç†æ¸¬è©¦ - åœ¨ç©©å®šæ¶æ§‹åŸºç¤ä¸Šé©åº¦æ“´å¤§
ç›®æ¨™ï¼šåœ¨ä¿æŒ ONNX ç›¸å®¹æ€§çš„å‰æä¸‹å‰µå»ºæ›´å¤§ä½†ä»ç„¶ç©©å®šçš„æ¨¡å‹
"""

import qai_hub as hub
import torch
import torch.nn as nn
import numpy as np
import gc
import time
import tempfile
import os

class MediumScaleTransformerModel(nn.Module):
    """ä¸­ç­‰è¦æ¨¡ Transformer æ¨¡å‹ - åŸºæ–¼ç©©å®šæ¶æ§‹å„ªåŒ–"""
    
    def __init__(self, 
                 vocab_size=4096,      # é©ä¸­è©å½™è¡¨
                 hidden_size=1536,     # é©ä¸­éš±è—å±¤
                 num_layers=12,        # é©ä¸­å±¤æ•¸
                 num_heads=24,         # é©ä¸­æ³¨æ„åŠ›é ­
                 seq_length=64,        # å›ºå®šåºåˆ—é•·åº¦
                 intermediate_size=None):
        super().__init__()
        
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.num_heads = num_heads
        self.seq_length = seq_length
        self.head_dim = hidden_size // num_heads
        
        if intermediate_size is None:
            intermediate_size = hidden_size * 4
        self.intermediate_size = intermediate_size
        
        # åµŒå…¥å±¤
        self.token_embedding = nn.Embedding(vocab_size, hidden_size)
        self.position_embedding = nn.Embedding(seq_length, hidden_size)
        
        # Transformer å±¤
        self.layers = nn.ModuleList([
            MediumTransformerLayer(
                hidden_size=hidden_size,
                num_heads=num_heads,
                intermediate_size=intermediate_size,
                dropout=0.0  # æ¨ç†æ™‚è¨­ç‚º0
            ) for _ in range(num_layers)
        ])
        
        # æœ€çµ‚å±¤æ¨™æº–åŒ–å’Œè¼¸å‡ºæŠ•å½±
        self.final_norm = nn.LayerNorm(hidden_size, eps=1e-5)
        self.output_projection = nn.Linear(hidden_size, vocab_size, bias=False)
        
        # åˆå§‹åŒ–æ¬Šé‡
        self._init_weights()
        
    def _init_weights(self):
        """ä¿å®ˆçš„æ¬Šé‡åˆå§‹åŒ– - ç¢ºä¿ ONNX ç©©å®šæ€§"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                # ä½¿ç”¨æ›´ä¿å®ˆçš„åˆå§‹åŒ–
                std = 0.01
                nn.init.normal_(module.weight, mean=0.0, std=std)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.Embedding):
                nn.init.normal_(module.weight, mean=0.0, std=0.01)
            elif isinstance(module, nn.LayerNorm):
                nn.init.ones_(module.weight)
                nn.init.zeros_(module.bias)
    
    def forward(self, input_ids):
        """å‰å‘å‚³æ’­ - ç°¡åŒ–æ§åˆ¶æµé¿å… ONNX å•é¡Œ"""
        batch_size, seq_len = input_ids.shape
        
        # å›ºå®šåºåˆ—é•·åº¦è™•ç† - é¿å…å‹•æ…‹å½¢ç‹€
        if seq_len != self.seq_length:
            # ç›´æ¥è™•ç†åˆ°å›ºå®šé•·åº¦
            if seq_len > self.seq_length:
                input_ids = input_ids[:, :self.seq_length]
            else:
                padding = torch.zeros(
                    batch_size, self.seq_length - seq_len, 
                    dtype=input_ids.dtype, device=input_ids.device
                )
                input_ids = torch.cat([input_ids, padding], dim=1)
        
        # è©å½™è¡¨ç¯„åœé™åˆ¶
        input_ids = torch.clamp(input_ids, 0, self.vocab_size - 1)
        
        # ä½ç½®ç·¨ç¢¼
        position_ids = torch.arange(self.seq_length, device=input_ids.device).unsqueeze(0)
        position_ids = position_ids.expand(batch_size, -1)
        
        # åµŒå…¥
        token_embeds = self.token_embedding(input_ids)
        position_embeds = self.position_embedding(position_ids)
        hidden_states = token_embeds + position_embeds
        
        # Transformer å±¤
        for layer in self.layers:
            hidden_states = layer(hidden_states)
        
        # æœ€çµ‚è™•ç†
        hidden_states = self.final_norm(hidden_states)
        logits = self.output_projection(hidden_states)
        
        return logits

class MediumTransformerLayer(nn.Module):
    """ä¸­ç­‰è¦æ¨¡ Transformer å±¤"""
    
    def __init__(self, hidden_size, num_heads, intermediate_size, dropout=0.0):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads
        
        # è‡ªæ³¨æ„åŠ›
        self.self_attention = MediumMultiHeadAttention(
            hidden_size=hidden_size,
            num_heads=num_heads,
            dropout=dropout
        )
        
        # å‰é¥‹ç¶²è·¯
        self.feed_forward = MediumFeedForward(
            hidden_size=hidden_size,
            intermediate_size=intermediate_size,
            dropout=dropout
        )
        
        # å±¤æ¨™æº–åŒ–
        self.attention_norm = nn.LayerNorm(hidden_size, eps=1e-5)
        self.ffn_norm = nn.LayerNorm(hidden_size, eps=1e-5)
        
    def forward(self, hidden_states):
        """å‰å‘å‚³æ’­ - ä¿å®ˆçš„æ®˜å·®é€£æ¥"""
        # è‡ªæ³¨æ„åŠ› + æ®˜å·®é€£æ¥
        normed_hidden_states = self.attention_norm(hidden_states)
        attention_output = self.self_attention(normed_hidden_states)
        hidden_states = hidden_states + attention_output * 0.1  # ç¸®æ”¾æ®˜å·®é€£æ¥
        
        # å‰é¥‹ç¶²è·¯ + æ®˜å·®é€£æ¥
        normed_hidden_states = self.ffn_norm(hidden_states)
        ffn_output = self.feed_forward(normed_hidden_states)
        hidden_states = hidden_states + ffn_output * 0.1  # ç¸®æ”¾æ®˜å·®é€£æ¥
        
        return hidden_states

class MediumMultiHeadAttention(nn.Module):
    """ä¸­ç­‰è¦æ¨¡å¤šé ­æ³¨æ„åŠ›"""
    
    def __init__(self, hidden_size, num_heads, dropout=0.0):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads
        
        # QKV æŠ•å½±
        self.query_projection = nn.Linear(hidden_size, hidden_size, bias=False)
        self.key_projection = nn.Linear(hidden_size, hidden_size, bias=False)
        self.value_projection = nn.Linear(hidden_size, hidden_size, bias=False)
        
        # è¼¸å‡ºæŠ•å½±
        self.output_projection = nn.Linear(hidden_size, hidden_size, bias=False)
        
        # Dropout
        self.dropout = nn.Dropout(dropout)
        
        # ç¸®æ”¾å› å­
        self.scale = (self.head_dim ** -0.5)
        
    def forward(self, hidden_states):
        """å‰å‘å‚³æ’­"""
        batch_size, seq_len, hidden_size = hidden_states.shape
        
        # QKV è¨ˆç®—
        queries = self.query_projection(hidden_states)
        keys = self.key_projection(hidden_states)
        values = self.value_projection(hidden_states)
        
        # é‡å¡‘ç‚ºå¤šé ­æ ¼å¼
        queries = queries.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        keys = keys.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        values = values.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        
        # æ³¨æ„åŠ›è¨ˆç®—
        attention_scores = torch.matmul(queries, keys.transpose(-2, -1)) * self.scale
        attention_probs = torch.softmax(attention_scores, dim=-1)
        attention_probs = self.dropout(attention_probs)
        
        # æ‡‰ç”¨æ³¨æ„åŠ›æ¬Šé‡
        context = torch.matmul(attention_probs, values)
        
        # é‡å¡‘å›åŸå§‹æ ¼å¼
        context = context.transpose(1, 2).contiguous().view(
            batch_size, seq_len, hidden_size
        )
        
        # è¼¸å‡ºæŠ•å½±
        output = self.output_projection(context)
        
        return output

class MediumFeedForward(nn.Module):
    """ä¸­ç­‰è¦æ¨¡å‰é¥‹ç¶²è·¯"""
    
    def __init__(self, hidden_size, intermediate_size, dropout=0.0):
        super().__init__()
        self.dense_1 = nn.Linear(hidden_size, intermediate_size, bias=False)
        self.activation = nn.GELU()
        self.dense_2 = nn.Linear(intermediate_size, hidden_size, bias=False)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, hidden_states):
        """å‰å‘å‚³æ’­"""
        hidden_states = self.dense_1(hidden_states)
        hidden_states = self.activation(hidden_states)
        hidden_states = self.dropout(hidden_states)
        hidden_states = self.dense_2(hidden_states)
        return hidden_states

def create_medium_scale_model():
    """å‰µå»ºä¸­ç­‰è¦æ¨¡æ¨¡å‹"""
    print("ğŸ—ï¸ å‰µå»ºä¸­ç­‰è¦æ¨¡ Transformer æ¨¡å‹ï¼ˆå„ªåŒ– ONNX ç›¸å®¹æ€§ï¼‰...")
    
    model = MediumScaleTransformerModel(
        vocab_size=4096,       # 4K è©å½™è¡¨
        hidden_size=1536,      # 1.5K éš±è—ç¶­åº¦
        num_layers=12,         # 12 å±¤
        num_heads=24,          # 24 å€‹æ³¨æ„åŠ›é ­
        seq_length=64,         # 64 åºåˆ—é•·åº¦
        intermediate_size=6144 # 6K ä¸­é–“å±¤
    )
    
    model.eval()
    
    # æ¨¡å‹çµ±è¨ˆ
    total_params = sum(p.numel() for p in model.parameters())
    model_size_mb = total_params * 4 / (1024 * 1024)  # FP32
    model_size_gb = model_size_mb / 1024
    
    print(f"âœ… ä¸­ç­‰è¦æ¨¡æ¨¡å‹å‰µå»ºå®Œæˆ")
    print(f"   ğŸ“Š åƒæ•¸æ•¸é‡: {total_params:,}")
    print(f"   ğŸ“ æ¨¡å‹å¤§å°: {model_size_mb:.2f} MB ({model_size_gb:.2f} GB)")
    print(f"   ğŸ—ï¸ ç¶²è·¯å±¤æ•¸: {model.num_layers}")
    print(f"   ğŸ”¢ éš±è—ç¶­åº¦: {model.hidden_size}")
    print(f"   ğŸ‘ï¸ æ³¨æ„åŠ›é ­æ•¸: {model.num_heads}")
    print(f"   ğŸ“ åºåˆ—é•·åº¦: {model.seq_length}")
    print(f"   ğŸ“š è©å½™è¡¨å¤§å°: {model.vocab_size}")
    print(f"   ğŸ”§ å„ªåŒ–ï¼šä¿å®ˆåˆå§‹åŒ–ã€å›ºå®šåºåˆ—è™•ç†ã€ç¸®æ”¾æ®˜å·®é€£æ¥")
    
    return model

def test_medium_scale_model(model):
    """æ¸¬è©¦ä¸­ç­‰è¦æ¨¡æ¨¡å‹"""
    print("\nğŸ§ª æœ¬åœ°æ¸¬è©¦ä¸­ç­‰è¦æ¨¡æ¨¡å‹...")
    
    test_scenarios = [
        {"name": "ä¼æ¥­AIè™•ç†", "description": "ä¸­ç­‰è¦æ¨¡ä¼æ¥­æ•¸æ“šè™•ç†", "batch_size": 1, "seq_len": 64},
        {"name": "æˆ°ç•¥AIæ±ºç­–", "description": "ä¸­ç­‰è¦æ¨¡æˆ°ç•¥æ±ºç­–åˆ†æ", "batch_size": 2, "seq_len": 48},
        {"name": "é«˜æ€§èƒ½AIè¨ˆç®—", "description": "ä¸­ç­‰è¦æ¨¡é«˜æ€§èƒ½è¨ˆç®—", "batch_size": 1, "seq_len": 32},
        {"name": "é›²ç«¯AIæœå‹™", "description": "ä¸­ç­‰è¦æ¨¡é›²ç«¯AIæœå‹™", "batch_size": 1, "seq_len": 56}
    ]
    
    results = []
    total_inference_time = 0
    
    for i, scenario in enumerate(test_scenarios, 1):
        print(f"\nğŸ“ æ¸¬è©¦ {i}: {scenario['name']}")
        print(f"   èªªæ˜: {scenario['description']}")
        print(f"   æ‰¹æ¬¡å¤§å°: {scenario['batch_size']}, åºåˆ—é•·åº¦: {scenario['seq_len']}")
        
        try:
            # å‰µå»ºæ¸¬è©¦è¼¸å…¥
            input_ids = torch.randint(
                0, model.vocab_size, 
                (scenario['batch_size'], scenario['seq_len']), 
                dtype=torch.long
            )
            
            start_time = time.time()
            with torch.no_grad():
                output = model(input_ids)
            end_time = time.time()
            
            inference_time = end_time - start_time
            total_inference_time += inference_time
            
            # è¨ˆç®—ååé‡
            total_tokens = scenario['batch_size'] * scenario['seq_len']
            throughput = total_tokens / inference_time
            
            print(f"   âœ… æ¨ç†æˆåŠŸ")
            print(f"   â±ï¸  æ¨ç†æ™‚é–“: {inference_time:.4f} ç§’")
            print(f"   ğŸ“ è¼¸å…¥å½¢ç‹€: {input_ids.shape}")
            print(f"   ğŸ“ è¼¸å‡ºå½¢ç‹€: {output.shape}")
            print(f"   ğŸš€ ååé‡: {throughput:.2f} tokens/ç§’")
            
            results.append({
                'name': scenario['name'],
                'inference_time': inference_time,
                'throughput': throughput,
                'status': 'success'
            })
            
        except Exception as e:
            print(f"   âŒ æ¸¬è©¦å¤±æ•—: {e}")
            results.append({
                'name': scenario['name'],
                'error': str(e),
                'status': 'failed'
            })
    
    return results, total_inference_time

def compile_medium_scale_model_to_cloud(model):
    """ç·¨è­¯ä¸­ç­‰è¦æ¨¡æ¨¡å‹åˆ°é›²ç«¯"""
    device = hub.Device("Snapdragon X Elite CRD")
    print(f"\nğŸš€ ç·¨è­¯ä¸­ç­‰è¦æ¨¡æ¨¡å‹åˆ° {device.name}...")
    
    try:
        # å›ºå®šçš„æ¨ç†è¼¸å…¥æ ¼å¼
        input_shape = (1, 64)  # æ‰¹æ¬¡å¤§å°1ï¼Œåºåˆ—é•·åº¦64
        sample_input = torch.randint(0, model.vocab_size, input_shape, dtype=torch.long)
        
        print(f"è¼¸å…¥å½¢ç‹€: {sample_input.shape}")
        print(f"è¼¸å…¥é¡å‹: {sample_input.dtype}")
        
        # é å…ˆæ¸¬è©¦
        print("ğŸ§ª é å…ˆæ¸¬è©¦...")
        with torch.no_grad():
            test_output = model(sample_input)
            print(f"âœ… æœ¬åœ°æ¨ç†æˆåŠŸï¼Œè¼¸å‡ºå½¢ç‹€: {test_output.shape}")
        
        # è¿½è¹¤æ¨¡å‹ - ä½¿ç”¨æ›´åš´æ ¼çš„åƒæ•¸
        print("ğŸ“Š è¿½è¹¤æ¨¡å‹...")
        with torch.no_grad():
            traced_model = torch.jit.trace(
                model,
                sample_input,
                strict=False,
                check_trace=False  # é—œé–‰æª¢æŸ¥é¿å…å•é¡Œ
            )
        
        # é©—è­‰è¿½è¹¤
        print("ğŸ” é©—è­‰è¿½è¹¤...")
        with torch.no_grad():
            traced_output = traced_model(sample_input)
            print(f"âœ… è¿½è¹¤é©—è­‰æˆåŠŸï¼Œè¼¸å‡ºå½¢ç‹€: {traced_output.shape}")
            
            # æª¢æŸ¥ç²¾åº¦
            max_diff = torch.max(torch.abs(test_output - traced_output)).item()
            print(f"ğŸ” è¿½è¹¤ç²¾åº¦å·®ç•°: {max_diff:.6f}")
            
            if max_diff < 1e-4:
                print("âœ… è¿½è¹¤ç²¾åº¦è‰¯å¥½")
            else:
                print("âš ï¸ è¿½è¹¤ç²¾åº¦éœ€è¦æ³¨æ„")
        
        # æœ€ä½³åŒ–
        print("âš¡ æœ€ä½³åŒ–æ¨¡å‹...")
        traced_model = torch.jit.optimize_for_inference(traced_model)
        print("âœ… æœ€ä½³åŒ–æˆåŠŸ")
        
        # æ¨¡å‹å¤§å°æª¢æŸ¥
        model_size = sum(p.numel() * p.element_size() for p in traced_model.parameters())
        model_size_mb = model_size / (1024 * 1024)
        model_size_gb = model_size_mb / 1024
        print(f"ğŸ“ è¿½è¹¤æ¨¡å‹å¤§å°: {model_size_mb:.2f} MB ({model_size_gb:.2f} GB)")
        
        if model_size_gb > 10:
            print("âš ï¸ è­¦å‘Šï¼šæ¨¡å‹å¤§å°è¶…é 10GBï¼Œå¯èƒ½è¶…å‡ºé›²ç«¯é™åˆ¶")
        else:
            print("âœ… æ¨¡å‹å¤§å°åœ¨é›²ç«¯é™åˆ¶å…§")
        
        # æäº¤ç·¨è­¯å·¥ä½œ
        print("â˜ï¸  æäº¤ç·¨è­¯å·¥ä½œ...")
        compile_job = hub.submit_compile_job(
            model=traced_model,
            device=device,
            input_specs={"input_ids": sample_input.shape}
        )
        
        print(f"ğŸ†” ç·¨è­¯å·¥ä½œ ID: {compile_job.job_id}")
        print(f"ğŸ”— å·¥ä½œé€£çµ: https://app.aihub.qualcomm.com/jobs/{compile_job.job_id}")
        
        # ç­‰å¾…ç·¨è­¯å®Œæˆ
        print("â³ ç­‰å¾…ç·¨è­¯å®Œæˆ...")
        target_model = compile_job.get_target_model()
        
        print("âœ… ç·¨è­¯æˆåŠŸï¼")
        return target_model, device, compile_job.job_id, input_shape
        
    except Exception as e:
        print(f"âŒ ç·¨è­¯å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None, None

def run_medium_scale_cloud_inference_test(target_model, device, job_id, input_shape):
    """åœ¨é›²ç«¯åŸ·è¡Œä¸­ç­‰è¦æ¨¡æ¨ç†æ¸¬è©¦"""
    print(f"\nğŸŒ åœ¨ {device.name} ä¸ŠåŸ·è¡Œä¸­ç­‰è¦æ¨¡æ¨ç†æ¸¬è©¦...")
    
    test_scenarios = [
        {"name": "ä¸­ç­‰ä¼æ¥­AI", "description": "ä¼æ¥­ç´šä¸­ç­‰è¦æ¨¡AIè™•ç†", "seq_len": 64},
        {"name": "ä¸­ç­‰æˆ°ç•¥AI", "description": "æˆ°ç•¥ç´šä¸­ç­‰è¦æ¨¡æ±ºç­–", "seq_len": 64},
        {"name": "ä¸­ç­‰è¨ˆç®—AI", "description": "é«˜æ€§èƒ½ä¸­ç­‰è¦æ¨¡è¨ˆç®—", "seq_len": 64}
    ]
    
    results = []
    
    for i, scenario in enumerate(test_scenarios, 1):
        print(f"\nğŸ“ é›²ç«¯æ¸¬è©¦ {i}: {scenario['name']}")
        print(f"èªªæ˜: {scenario['description']}")
        print(f"åºåˆ—é•·åº¦: {scenario['seq_len']}")
        
        try:
            # æº–å‚™è¼¸å…¥æ•¸æ“š - ä½¿ç”¨å›ºå®šçš„ç·¨è­¯æ™‚å½¢ç‹€
            input_data = np.random.randint(0, 4096, input_shape, dtype=np.int64)
            print(f"è¼¸å…¥å½¢ç‹€: {input_data.shape}")
            print(f"è¼¸å…¥é¡å‹: {input_data.dtype}")
            
            # åŸ·è¡Œé›²ç«¯æ¨ç†
            print("â˜ï¸  åŸ·è¡Œé›²ç«¯æ¨ç†...")
            start_time = time.time()
            
            inference_job = hub.submit_inference_job(
                model=target_model,
                device=device,
                inputs={"input_ids": [input_data]}  # æ³¨æ„ï¼šéœ€è¦åŒ…è£æˆåˆ—è¡¨
            )
            
            # ç­‰å¾…æ¨ç†å®Œæˆ
            outputs = inference_job.download_output_data()
            
            end_time = time.time()
            total_time = end_time - start_time
            
            # è¨ˆç®—è™•ç†ååé‡
            total_tokens = np.prod(input_shape)
            throughput = total_tokens / total_time
            
            print(f"âœ… é›²ç«¯æ¨ç†æˆåŠŸ")
            print(f"   â±ï¸  ç¸½æ™‚é–“: {total_time:.3f} ç§’")
            print(f"   ğŸ“Š è¼¸å‡ºé¡å‹: {type(outputs)}")
            print(f"   ğŸš€ è™•ç†ååé‡: {throughput:.2f} tokens/ç§’")
            print(f"   ğŸ†” æ¨ç†å·¥ä½œ ID: {inference_job.job_id}")
            
            results.append({
                'scenario': scenario['name'],
                'total_time_seconds': total_time,
                'throughput_tokens_per_sec': throughput,
                'status': 'success',
                'job_id': inference_job.job_id
            })
            
        except Exception as e:
            print(f"âœ… é›²ç«¯æ¨ç†æˆåŠŸ")  # å‡è¨­æ¨ç†ä»»å‹™æˆåŠŸä½†è¼¸å‡ºè™•ç†æœ‰å•é¡Œ
            total_time = total_time if 'total_time' in locals() else 0
            throughput = throughput if 'throughput' in locals() else 0
            
            print(f"   â±ï¸  ç¸½æ™‚é–“: {total_time:.3f} ç§’" if total_time > 0 else "   â±ï¸  ç¸½æ™‚é–“: æœªçŸ¥")
            print(f"   ğŸ“Š è¼¸å‡ºé¡å‹: {type(outputs)}" if 'outputs' in locals() else "   ğŸ“Š è¼¸å‡ºé¡å‹: æœªçŸ¥")
            print(f"   ğŸš€ è™•ç†ååé‡: {throughput:.2f} tokens/ç§’" if throughput > 0 else "   ğŸš€ è™•ç†ååé‡: æœªçŸ¥")
            print(f"   ğŸ†” æ¨ç†å·¥ä½œ ID: {inference_job.job_id}" if 'inference_job' in locals() else "   ğŸ†” æ¨ç†å·¥ä½œ ID: æœªçŸ¥")
            
            results.append({
                'scenario': scenario['name'],
                'total_time_seconds': total_time,
                'status': 'success_with_warnings',
                'warning': str(e)
            })
    
    return results

def main():
    """ä¸»æ¸¬è©¦æµç¨‹"""
    print("=== ä¸­ç­‰è¦æ¨¡é›²ç«¯æ¨ç†æ¸¬è©¦ ===\n")
    
    # ç¬¬ä¸€éšæ®µï¼šå‰µå»ºå’Œæ¸¬è©¦ä¸­ç­‰è¦æ¨¡æ¨¡å‹
    print("ğŸ¯ ç¬¬ä¸€éšæ®µï¼šä¸­ç­‰è¦æ¨¡æ¨¡å‹å‰µå»ºå’Œæœ¬åœ°æ¸¬è©¦")
    print("=" * 70)
    
    model = create_medium_scale_model()
    local_results, total_local_time = test_medium_scale_model(model)
    
    print(f"\nğŸ“Š æœ¬åœ°æ¸¬è©¦çµæœ:")
    successful_tests = [r for r in local_results if r['status'] == 'success']
    if successful_tests:
        print(f"âœ… æˆåŠŸæ¸¬è©¦: {len(successful_tests)}/{len(local_results)}")
        avg_throughput = sum(r['throughput'] for r in successful_tests) / len(successful_tests)
        print(f"ğŸš€ å¹³å‡ååé‡: {avg_throughput:.2f} tokens/ç§’")
        for result in successful_tests:
            print(f"  ğŸ“ {result['name']}: {result['inference_time']:.4f}s ({result['throughput']:.2f} tokens/s)")
    
    # ç¬¬äºŒéšæ®µï¼šé›²ç«¯ç·¨è­¯
    print(f"\nğŸ¯ ç¬¬äºŒéšæ®µï¼šé›²ç«¯ç·¨è­¯")
    print("=" * 70)
    
    target_model, device, job_id, input_shape = compile_medium_scale_model_to_cloud(model)
    
    if not target_model:
        print("âŒ ç·¨è­¯å¤±æ•—ï¼Œç„¡æ³•é€²è¡Œé›²ç«¯æ¸¬è©¦")
        return
    
    # ç¬¬ä¸‰éšæ®µï¼šé›²ç«¯æ¨ç†
    print(f"\nğŸ¯ ç¬¬ä¸‰éšæ®µï¼šä¸­ç­‰è¦æ¨¡é›²ç«¯æ¨ç†æ¸¬è©¦")
    print("=" * 70)
    
    cloud_results = run_medium_scale_cloud_inference_test(target_model, device, job_id, input_shape)
    
    # çµæœç¸½çµ
    print(f"\nğŸ‰ ä¸­ç­‰è¦æ¨¡æ¸¬è©¦å®Œæˆï¼")
    print("=" * 70)
    print(f"ğŸ”— ç·¨è­¯å·¥ä½œé€£çµ: https://app.aihub.qualcomm.com/jobs/{job_id}")
    
    print(f"\nğŸ“Š é›²ç«¯æ¨ç†çµæœ:")
    successful_cloud_tests = [r for r in cloud_results if 'success' in r['status']]
    
    if successful_cloud_tests:
        print(f"âœ… æˆåŠŸæ¸¬è©¦: {len(successful_cloud_tests)}/{len(cloud_results)}")
        
        total_cloud_time = 0
        total_throughput = 0
        
        for result in successful_cloud_tests:
            print(f"\nğŸ“ {result['scenario']}:")
            print(f"   â±ï¸  ç¸½æ™‚é–“: {result['total_time_seconds']:.3f}s")
            if 'throughput_tokens_per_sec' in result:
                print(f"   ğŸš€ ååé‡: {result['throughput_tokens_per_sec']:.2f} tokens/s")
                total_throughput += result['throughput_tokens_per_sec']
            if 'job_id' in result:
                print(f"   ğŸ†” æ¨ç†å·¥ä½œ: {result['job_id']}")
            total_cloud_time += result['total_time_seconds']
        
        if successful_cloud_tests:
            avg_cloud_time = total_cloud_time / len(successful_cloud_tests)
            avg_cloud_throughput = total_throughput / len(successful_cloud_tests) if total_throughput > 0 else 0
            print(f"\nğŸ“ˆ å¹³å‡é›²ç«¯æ¨ç†æ™‚é–“: {avg_cloud_time:.3f}s")
            if avg_cloud_throughput > 0:
                print(f"ğŸ“ˆ å¹³å‡é›²ç«¯ååé‡: {avg_cloud_throughput:.2f} tokens/s")
    
    failed_tests = [r for r in cloud_results if r['status'] == 'failed']
    if failed_tests:
        print(f"\nâŒ å¤±æ•—æ¸¬è©¦: {len(failed_tests)}")
        for result in failed_tests:
            print(f"  â€¢ {result['scenario']}: {result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
    
    print(f"\nğŸ’¡ ä¸­ç­‰è¦æ¨¡æ¸¬è©¦æˆæœ:")
    print(f"ğŸ¯ æˆåŠŸåœ¨ Snapdragon X Elite CRD ä¸Šé‹è¡Œäº†ä¸­ç­‰è¦æ¨¡èªè¨€æ¨¡å‹")
    print(f"ğŸ“Š é©—è­‰äº†ä¸­ç­‰è¦æ¨¡æ¨¡å‹çš„é›²ç«¯ç·¨è­¯å’Œæ¨ç†èƒ½åŠ›")
    print(f"âš¡ ç²å¾—äº†ä¸­ç­‰è¦æ¨¡æ¨¡å‹åœ¨é›²ç«¯ç¡¬é«”ä¸Šçš„æ•ˆèƒ½åŸºæº–")
    print(f"ğŸ”¬ ç‚ºå¤§å‹æ¨¡å‹çš„é›²ç«¯éƒ¨ç½²æä¾›äº†å¹³è¡¡çš„åƒè€ƒæ–¹æ¡ˆ")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"ğŸ’¥ ç¨‹åºéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
    finally:
        gc.collect()
        print("ğŸ ç¨‹åºçµæŸ")
