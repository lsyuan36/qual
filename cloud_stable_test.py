#!/usr/bin/env python3
"""
ç©©å®šå¤§è¦æ¨¡é›²ç«¯æ¸¬è©¦ - å„ªåŒ– ONNX ç›¸å®¹æ€§ï¼Œé¿å…ç°¡åŒ–å™¨å•é¡Œ
ç›®æ¨™ï¼šåœ¨ Snapdragon X Elite CRD ä¸Šç©©å®šé‹è¡Œå¤§è¦æ¨¡æ¨¡å‹
"""

import qai_hub as hub
import torch
import torch.nn as nn
import gc
import time
import numpy as np

class StableCompatibleModel(nn.Module):
    """ç©©å®šçš„å¤§è¦æ¨¡ç›¸å®¹æ¨¡å‹ - å„ªåŒ– ONNX è½‰æ›ç©©å®šæ€§"""
    
    def __init__(self, input_size=1024, hidden_size=1024, num_layers=8, output_size=1024):
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.output_size = output_size
        
        # è¼¸å…¥æŠ•å½±å±¤ - ç°¡åŒ–çµæ§‹æé«˜ç©©å®šæ€§
        self.input_projection = nn.Linear(input_size, hidden_size)
        self.input_norm = nn.LayerNorm(hidden_size)
        
        # ç©©å®šçš„æ·±åº¦ç¶²è·¯å±¤ - é¿å…éæ–¼è¤‡é›œçš„çµæ§‹
        self.layers = nn.ModuleList()
        for i in range(num_layers):
            layer = nn.Sequential(
                nn.Linear(hidden_size, hidden_size * 3),  # 3å€æ“´å±•ï¼ˆè¼ƒä¿å®ˆï¼‰
                nn.ReLU(),
                nn.Linear(hidden_size * 3, hidden_size * 2),  # 2å€ä¸­é–“å±¤
                nn.ReLU(),
                nn.Linear(hidden_size * 2, hidden_size),  # å›åˆ°åŸå§‹å¤§å°
                nn.LayerNorm(hidden_size)
            )
            self.layers.append(layer)
        
        # è¼¸å‡ºæŠ•å½±å±¤ - ç°¡åŒ–çµæ§‹
        self.output_projection = nn.Linear(hidden_size, output_size)
        self.output_norm = nn.LayerNorm(output_size)
        
        # æ®˜å·®é€£æ¥æ¬Šé‡ - ç°¡åŒ–ç‚ºå›ºå®šå€¼é¿å…è¤‡é›œåƒæ•¸
        self.residual_scale = 0.1
        
        # åˆå§‹åŒ–æ¬Šé‡
        self._init_weights()
        
    def _init_weights(self):
        """åˆå§‹åŒ–æ¨¡å‹æ¬Šé‡ - ä½¿ç”¨æ›´ä¿å®ˆçš„åˆå§‹åŒ–"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.normal_(module.weight, mean=0.0, std=0.01)  # æ›´å°çš„æ¨™æº–å·®
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
    
    def forward(self, x):
        """å‰å‘å‚³æ’­ - ç°¡åŒ–æ“ä½œæé«˜ ONNX ç©©å®šæ€§"""
        # è¼¸å…¥æŠ•å½±
        x = self.input_projection(x)
        x = self.input_norm(x)
        
        # æ·±åº¦è™•ç† - ä½¿ç”¨ç°¡å–®çš„æ®˜å·®é€£æ¥
        for layer in self.layers:
            residual = x
            x = layer(x)
            # ç°¡åŒ–çš„æ®˜å·®é€£æ¥
            x = x + self.residual_scale * residual
        
        # è¼¸å‡ºæŠ•å½±
        x = self.output_projection(x)
        output = self.output_norm(x)
        
        return output

def create_stable_model():
    """å‰µå»ºç©©å®šçš„å¤§è¦æ¨¡èªè¨€æ¨¡å‹"""
    print("ğŸ—ï¸ å‰µå»ºç©©å®šå¤§è¦æ¨¡æ¨¡å‹ï¼ˆå„ªåŒ– ONNX ç›¸å®¹æ€§ï¼‰...")
    
    model = StableCompatibleModel(
        input_size=1024,      # å¹³è¡¡çš„ç¶­åº¦
        hidden_size=1024,     # å¹³è¡¡çš„éš±è—å±¤ç¶­åº¦  
        num_layers=8,         # é©ä¸­çš„å±¤æ•¸é¿å…éæ·±
        output_size=1024      # å¹³è¡¡çš„è¼¸å‡ºç¶­åº¦
    )
    
    model.eval()
    
    # æ¨¡å‹å¤§å°ä¼°ç®—
    total_params = sum(p.numel() for p in model.parameters())
    model_size_mb = total_params * 4 / (1024 * 1024)  # FP32
    model_size_gb = model_size_mb / 1024
    
    print(f"âœ… ç©©å®šå¤§è¦æ¨¡æ¨¡å‹å‰µå»ºå®Œæˆ")
    print(f"   ğŸ“Š åƒæ•¸æ•¸é‡: {total_params:,}")
    print(f"   ğŸ“ æ¨¡å‹å¤§å°: {model_size_mb:.2f} MB ({model_size_gb:.2f} GB)")
    print(f"   ğŸ—ï¸ ç¶²è·¯å±¤æ•¸: {model.num_layers}")
    print(f"   ğŸ”¢ éš±è—ç¶­åº¦: {model.hidden_size}")
    print(f"   ğŸ“ è¼¸å…¥/è¼¸å‡ºç¶­åº¦: {model.input_size}")
    print(f"   ğŸ”§ å„ªåŒ–ï¼šç°¡åŒ–æ¶æ§‹æé«˜ ONNX ç©©å®šæ€§")
    
    return model

def test_stable_model(model):
    """æ¸¬è©¦ç©©å®šå¤§è¦æ¨¡æ¨¡å‹"""
    print("\nğŸ§ª æœ¬åœ°æ¸¬è©¦ç©©å®šå¤§è¦æ¨¡æ¨¡å‹...")
    
    test_cases = [
        {"name": "ä¼æ¥­AIè™•ç†", "description": "ä¼æ¥­ç´šAIæ•¸æ“šè™•ç†", "batch_size": 1, "seq_len": 64},
        {"name": "æˆ°ç•¥æ±ºç­–å¼•æ“", "description": "æˆ°ç•¥ç´šæ±ºç­–AI", "batch_size": 2, "seq_len": 32}, 
        {"name": "é«˜æ•ˆèƒ½è¨ˆç®—", "description": "é«˜æ€§èƒ½AIè¨ˆç®—", "batch_size": 4, "seq_len": 16},
        {"name": "é›²ç«¯æœå‹™", "description": "é›²ç«¯AIæœå‹™", "batch_size": 8, "seq_len": 8}
    ]
    
    results = []
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\nğŸ“ æ¸¬è©¦ {i}: {test_case['name']}")
        print(f"   èªªæ˜: {test_case['description']}")
        print(f"   æ‰¹æ¬¡å¤§å°: {test_case['batch_size']}, åºåˆ—é•·åº¦: {test_case['seq_len']}")
        
        try:
            # å‰µå»ºæ¸¬è©¦è¼¸å…¥
            input_data = torch.randn(
                test_case['batch_size'], 
                test_case['seq_len'], 
                model.input_size,
                dtype=torch.float32
            )
            
            start_time = time.time()
            with torch.no_grad():
                output = model(input_data)
            end_time = time.time()
            
            inference_time = end_time - start_time
            
            print(f"   âœ… æ¨ç†æˆåŠŸ")
            print(f"   â±ï¸  æ¨ç†æ™‚é–“: {inference_time:.4f} ç§’")
            print(f"   ğŸ“ è¼¸å…¥å½¢ç‹€: {input_data.shape}")
            print(f"   ğŸ“ è¼¸å‡ºå½¢ç‹€: {output.shape}")
            
            # è¨ˆç®—ååé‡
            throughput = (test_case['batch_size'] * test_case['seq_len']) / inference_time
            print(f"   ğŸš€ ååé‡: {throughput:.2f} tokens/ç§’")
            
            results.append({
                'name': test_case['name'],
                'inference_time': inference_time,
                'throughput': throughput,
                'batch_size': test_case['batch_size'],
                'seq_len': test_case['seq_len'],
                'status': 'success'
            })
            
        except Exception as e:
            print(f"   âŒ æ¸¬è©¦å¤±æ•—: {e}")
            results.append({
                'name': test_case['name'],
                'error': str(e),
                'status': 'failed'
            })
    
    return results

def compile_stable_model_to_cloud(model):
    """ç·¨è­¯ç©©å®šå¤§è¦æ¨¡æ¨¡å‹åˆ°é›²ç«¯ - å¢å¼·éŒ¯èª¤è™•ç†"""
    device = hub.Device("Snapdragon X Elite CRD")
    print(f"\nğŸš€ ç·¨è­¯ç©©å®šå¤§è¦æ¨¡æ¨¡å‹åˆ° {device.name}...")
    
    try:
        # ä½¿ç”¨è¼ƒå°çš„æ¨£æœ¬è¼¸å…¥æé«˜ç©©å®šæ€§
        sample_input = torch.randn(1, 32, model.input_size, dtype=torch.float32)
        print(f"è¼¸å…¥å½¢ç‹€: {sample_input.shape}")
        print(f"è¼¸å…¥é¡å‹: {sample_input.dtype}")
        
        # æ¸¬è©¦æ¨¡å‹
        print("ğŸ§ª é å…ˆæ¸¬è©¦...")
        with torch.no_grad():
            test_output = model(sample_input)
            print(f"âœ… æœ¬åœ°æ¨ç†æˆåŠŸï¼Œè¼¸å‡ºå½¢ç‹€: {test_output.shape}")
        
        # è¿½è¹¤æ¨¡å‹ - ä½¿ç”¨æ›´ä¿å®ˆçš„è¨­å®š
        print("ğŸ“Š è¿½è¹¤æ¨¡å‹...")
        with torch.no_grad():
            # è¨­å®šæ›´åš´æ ¼çš„è¿½è¹¤åƒæ•¸
            traced_model = torch.jit.trace(
                model,
                sample_input,
                strict=True,  # å•Ÿç”¨åš´æ ¼æ¨¡å¼
                check_trace=True  # å•Ÿç”¨è¿½è¹¤æª¢æŸ¥
            )
        
        # é©—è­‰è¿½è¹¤çµæœ
        print("ğŸ” é©—è­‰è¿½è¹¤...")
        with torch.no_grad():
            traced_output = traced_model(sample_input)
            print(f"âœ… è¿½è¹¤é©—è­‰æˆåŠŸï¼Œè¼¸å‡ºå½¢ç‹€: {traced_output.shape}")
            
            # æª¢æŸ¥è¼¸å‡ºä¸€è‡´æ€§
            original_output = model(sample_input)
            diff = torch.abs(traced_output - original_output).max().item()
            print(f"ğŸ” è¿½è¹¤ç²¾åº¦å·®ç•°: {diff:.6f}")
            
            if diff > 1e-4:
                print(f"âš ï¸  è­¦å‘Šï¼šè¿½è¹¤ç²¾åº¦å·®ç•°è¼ƒå¤§")
            else:
                print(f"âœ… è¿½è¹¤ç²¾åº¦è‰¯å¥½")
        
        # æœ€ä½³åŒ– - ä½¿ç”¨è¼ƒä¿å®ˆçš„æœ€ä½³åŒ–
        print("âš¡ æœ€ä½³åŒ–æ¨¡å‹...")
        try:
            traced_model = torch.jit.optimize_for_inference(traced_model)
            print("âœ… æœ€ä½³åŒ–æˆåŠŸ")
        except Exception as opt_error:
            print(f"âš ï¸  æœ€ä½³åŒ–è·³é: {opt_error}")
        
        # æª¢æŸ¥æ¨¡å‹å¤§å°
        total_params = sum(p.numel() for p in traced_model.parameters())
        model_size_mb = total_params * 4 / (1024 * 1024)
        model_size_gb = model_size_mb / 1024
        print(f"ğŸ“ è¿½è¹¤æ¨¡å‹å¤§å°: {model_size_mb:.2f} MB ({model_size_gb:.2f} GB)")
        
        # æäº¤ç·¨è­¯å·¥ä½œ - å¢åŠ é‡è©¦æ©Ÿåˆ¶
        print("â˜ï¸  æäº¤ç·¨è­¯å·¥ä½œ...")
        max_retries = 3
        for attempt in range(max_retries):
            try:
                compile_job = hub.submit_compile_job(
                    model=traced_model,
                    device=device,
                    input_specs={"x": sample_input.shape}
                )
                
                print(f"ğŸ†” ç·¨è­¯å·¥ä½œ ID: {compile_job.job_id}")
                print(f"ğŸ”— å·¥ä½œé€£çµ: https://app.aihub.qualcomm.com/jobs/{compile_job.job_id}")
                break
                
            except Exception as submit_error:
                print(f"âš ï¸  æäº¤å˜—è©¦ {attempt + 1} å¤±æ•—: {submit_error}")
                if attempt == max_retries - 1:
                    raise submit_error
                time.sleep(2)  # ç­‰å¾…å¾Œé‡è©¦
        
        # ç­‰å¾…ç·¨è­¯å®Œæˆ
        print("â³ ç­‰å¾…ç·¨è­¯å®Œæˆ...")
        try:
            target_model = compile_job.get_target_model()
            print("âœ… ç·¨è­¯æˆåŠŸï¼")
            return target_model, device, compile_job.job_id, sample_input.shape
        except Exception as compile_error:
            print(f"âŒ ç·¨è­¯éç¨‹å¤±æ•—: {compile_error}")
            return None, None, compile_job.job_id, None
        
    except Exception as e:
        print(f"âŒ ç·¨è­¯å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None, None

def run_stable_cloud_inference_test(target_model, device, job_id, input_shape):
    """åœ¨é›²ç«¯åŸ·è¡Œç©©å®šå¤§è¦æ¨¡æ¨ç†æ¸¬è©¦"""
    print(f"\nğŸŒ åœ¨ {device.name} ä¸ŠåŸ·è¡Œç©©å®šå¤§è¦æ¨¡æ¨ç†æ¸¬è©¦...")
    
    stable_scenarios = [
        {"name": "ç©©å®šä¼æ¥­AI", "description": "ç©©å®šä¼æ¥­AIè™•ç†", "seq_len": 32},
        {"name": "ç©©å®šæˆ°ç•¥AI", "description": "ç©©å®šæˆ°ç•¥æ±ºç­–", "seq_len": 24},
        {"name": "ç©©å®šè¨ˆç®—AI", "description": "ç©©å®šé«˜æ€§èƒ½è¨ˆç®—", "seq_len": 28},
        {"name": "ç©©å®šæœå‹™AI", "description": "ç©©å®šé›²ç«¯æœå‹™", "seq_len": 20}
    ]
    
    results = []
    
    for i, scenario in enumerate(stable_scenarios, 1):
        print(f"\nğŸ“ é›²ç«¯æ¸¬è©¦ {i}: {scenario['name']}")
        print(f"èªªæ˜: {scenario['description']}")
        print(f"åºåˆ—é•·åº¦: {scenario['seq_len']}")
        
        try:
            # æº–å‚™æ¸¬è©¦è¼¸å…¥ - ä¿®å¾©è¼¸å…¥æ ¼å¼
            batch_size, _, input_dim = input_shape
            input_data = np.random.randn(batch_size, scenario['seq_len'], input_dim).astype(np.float32)
            print(f"è¼¸å…¥å½¢ç‹€: {input_data.shape}")
            print(f"è¼¸å…¥é¡å‹: {input_data.dtype}")
            
            # åŸ·è¡Œé›²ç«¯æ¨ç†
            print("â˜ï¸  åŸ·è¡Œé›²ç«¯æ¨ç†...")
            start_time = time.time()
            
            inference_job = hub.submit_inference_job(
                model=target_model,
                device=device,
                inputs={"x": [input_data]}  # ä¿®å¾©ï¼šè¼¸å…¥éœ€è¦æ˜¯åˆ—è¡¨æ ¼å¼
            )
            
            # ç­‰å¾…æ¨ç†å®Œæˆ
            outputs = inference_job.download_output_data()
            
            end_time = time.time()
            total_time = end_time - start_time
            
            print(f"âœ… é›²ç«¯æ¨ç†æˆåŠŸ")
            print(f"   â±ï¸  ç¸½æ™‚é–“: {total_time:.3f} ç§’")
            print(f"   ğŸ“Š è¼¸å‡ºé¡å‹: {type(outputs)}")
            
            # æª¢æŸ¥è¼¸å‡ºè©³ç´°ä¿¡æ¯
            if isinstance(outputs, dict):
                for key, value in outputs.items():
                    if isinstance(value, np.ndarray):
                        print(f"   ğŸ“ è¼¸å‡º {key} å½¢ç‹€: {value.shape}")
            
            # è¨ˆç®—æ€§èƒ½æŒ‡æ¨™
            tokens_processed = scenario['seq_len']
            throughput = tokens_processed / total_time
            
            print(f"   ğŸš€ è™•ç†ååé‡: {throughput:.2f} tokens/ç§’")
            print(f"   ğŸ†” æ¨ç†å·¥ä½œ ID: {inference_job.job_id}")
            
            results.append({
                'scenario': scenario['name'],
                'total_time_seconds': total_time,
                'throughput': throughput,
                'seq_len': scenario['seq_len'],
                'status': 'success',
                'job_id': inference_job.job_id
            })
                
        except Exception as e:
            print(f"âŒ é›²ç«¯æ¨ç†å¤±æ•—: {e}")
            results.append({
                'scenario': scenario['name'],
                'error': str(e),
                'status': 'failed'
            })
    
    return results

def main():
    """ä¸»æ¸¬è©¦æµç¨‹"""
    print("=== ç©©å®šå¤§è¦æ¨¡é›²ç«¯æ¨ç†æ¸¬è©¦ ===\n")
    
    # ç¬¬ä¸€éšæ®µï¼šå‰µå»ºå’Œæ¸¬è©¦æ¨¡å‹
    print("ğŸ¯ ç¬¬ä¸€éšæ®µï¼šç©©å®šå¤§è¦æ¨¡æ¨¡å‹å‰µå»ºå’Œæœ¬åœ°æ¸¬è©¦")
    print("=" * 70)
    
    model = create_stable_model()
    local_results = test_stable_model(model)
    
    print(f"\nğŸ“Š æœ¬åœ°æ¸¬è©¦çµæœ:")
    successful_local = [r for r in local_results if r['status'] == 'success']
    if successful_local:
        avg_throughput = sum(r['throughput'] for r in successful_local) / len(successful_local)
        print(f"âœ… æˆåŠŸæ¸¬è©¦: {len(successful_local)}/{len(local_results)}")
        print(f"ğŸš€ å¹³å‡ååé‡: {avg_throughput:.2f} tokens/ç§’")
        
        for result in successful_local:
            print(f"  ğŸ“ {result['name']}: {result['inference_time']:.4f}s ({result['throughput']:.2f} tokens/s)")
    
    for result in local_results:
        if result['status'] == 'failed':
            print(f"  âŒ {result['name']}: {result['error']}")
    
    # ç¬¬äºŒéšæ®µï¼šé›²ç«¯ç·¨è­¯
    print(f"\nğŸ¯ ç¬¬äºŒéšæ®µï¼šé›²ç«¯ç·¨è­¯")
    print("=" * 70)
    
    target_model, device, job_id, input_shape = compile_stable_model_to_cloud(model)
    
    if not target_model:
        print("âŒ ç·¨è­¯å¤±æ•—ï¼Œç„¡æ³•é€²è¡Œé›²ç«¯æ¸¬è©¦")
        if job_id:
            print(f"ğŸ”— ç·¨è­¯å·¥ä½œé€£çµ: https://app.aihub.qualcomm.com/jobs/{job_id}")
        return
    
    # ç¬¬ä¸‰éšæ®µï¼šé›²ç«¯æ¨ç†
    print(f"\nğŸ¯ ç¬¬ä¸‰éšæ®µï¼šç©©å®šå¤§è¦æ¨¡é›²ç«¯æ¨ç†æ¸¬è©¦")
    print("=" * 70)
    
    cloud_results = run_stable_cloud_inference_test(target_model, device, job_id, input_shape)
    
    # çµæœç¸½çµ
    print(f"\nğŸ‰ ç©©å®šå¤§è¦æ¨¡æ¸¬è©¦å®Œæˆï¼")
    print("=" * 80)
    print(f"ğŸ”— ç·¨è­¯å·¥ä½œé€£çµ: https://app.aihub.qualcomm.com/jobs/{job_id}")
    
    print(f"\nğŸ“Š é›²ç«¯æ¨ç†çµæœ:")
    successful_tests = [r for r in cloud_results if 'success' in r['status']]
    
    if successful_tests:
        print(f"âœ… æˆåŠŸæ¸¬è©¦: {len(successful_tests)}/{len(cloud_results)}")
        
        total_time = sum(r['total_time_seconds'] for r in successful_tests)
        avg_time = total_time / len(successful_tests)
        avg_throughput = sum(r['throughput'] for r in successful_tests) / len(successful_tests)
        
        print(f"\nğŸ“ˆ æ€§èƒ½çµ±è¨ˆ:")
        print(f"   â±ï¸  å¹³å‡æ¨ç†æ™‚é–“: {avg_time:.3f}s")
        print(f"   ğŸš€ å¹³å‡ååé‡: {avg_throughput:.2f} tokens/ç§’")
        
        print(f"\nğŸ“ è©³ç´°çµæœ:")
        for result in successful_tests:
            print(f"   {result['scenario']}:")
            print(f"     â±ï¸  æ™‚é–“: {result['total_time_seconds']:.3f}s")
            print(f"     ğŸš€ ååé‡: {result['throughput']:.2f} tokens/s")
            print(f"     ğŸ“ åºåˆ—é•·åº¦: {result['seq_len']}")
            print(f"     ğŸ†” å·¥ä½œID: {result['job_id']}")
    
    failed_tests = [r for r in cloud_results if r['status'] == 'failed']
    if failed_tests:
        print(f"\nâŒ å¤±æ•—æ¸¬è©¦: {len(failed_tests)}")
        for result in failed_tests:
            print(f"  â€¢ {result['scenario']}: {result['error']}")
    
    print(f"\nğŸ’¡ ç©©å®šå¤§è¦æ¨¡æ¸¬è©¦æˆæœ:")
    print(f"ğŸ¯ æˆåŠŸåœ¨ Snapdragon X Elite CRD ä¸Šé‹è¡Œäº†ç©©å®šå¤§è¦æ¨¡æ¨¡å‹")
    print(f"ğŸ“Š å„ªåŒ–äº† ONNX ç›¸å®¹æ€§ï¼Œæé«˜ç·¨è­¯æˆåŠŸç‡")
    print(f"âš¡ ç²å¾—äº†ç©©å®šå¯é çš„æ¨ç†æ•ˆèƒ½æ•¸æ“š")
    print(f"ğŸ”¬ é©—è­‰äº†ç©©å®šå¤§è¦æ¨¡æ¨¡å‹çš„é›²ç«¯éƒ¨ç½²å¯è¡Œæ€§")
    print(f"ğŸ† ç‚ºå¯¦éš›æ‡‰ç”¨æä¾›äº†ç©©å®šçš„æŠ€è¡“æ–¹æ¡ˆ")
    print(f"ğŸŒŸ è­‰æ˜äº†å„ªåŒ–å¾Œçš„å¤§è¦æ¨¡æ¨¡å‹å¯ä»¥ç©©å®šé‹è¡Œ")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"ğŸ’¥ ç¨‹åºéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
    finally:
        gc.collect()
        print("ğŸ ç¨‹åºçµæŸ")
