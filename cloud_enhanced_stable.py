#!/usr/bin/env python3
"""
å¤§è¦æ¨¡ç©©å®šé›²ç«¯æ¸¬è©¦ - åŸºæ–¼æˆåŠŸæ¶æ§‹å‰µå»ºæ›´å¤§æ¨¡å‹
ç›®æ¨™ï¼šåœ¨ Snapdragon X Elite CRD ä¸Šç©©å®šé‹è¡Œæ›´å¤§è¦æ¨¡æ¨¡å‹
"""

import qai_hub as hub
import torch
import torch.nn as nn
import gc
import time
import numpy as np

class EnhancedStableModel(nn.Module):
    """å¢å¼·ç©©å®šæ¨¡å‹ - åŸºæ–¼æˆåŠŸæ¶æ§‹æ“´å¤§è¦æ¨¡"""
    
    def __init__(self, input_size=1536, hidden_size=1536, num_layers=12, output_size=1536):
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.output_size = output_size
        
        # è¼¸å…¥æŠ•å½±å±¤ - ä¿æŒç°¡åŒ–çµæ§‹
        self.input_projection = nn.Linear(input_size, hidden_size)
        self.input_norm = nn.LayerNorm(hidden_size)
        
        # å¢å¼·çš„æ·±åº¦ç¶²è·¯å±¤ - åŸºæ–¼æˆåŠŸæ¶æ§‹
        self.layers = nn.ModuleList()
        for i in range(num_layers):
            layer = nn.Sequential(
                nn.Linear(hidden_size, hidden_size * 3),  # 3å€æ“´å±•
                nn.ReLU(),
                nn.Linear(hidden_size * 3, hidden_size * 2),  # 2å€ä¸­é–“å±¤
                nn.ReLU(),
                nn.Linear(hidden_size * 2, hidden_size),  # å›åˆ°åŸå§‹å¤§å°
                nn.LayerNorm(hidden_size)
            )
            self.layers.append(layer)
        
        # è¼¸å‡ºæŠ•å½±å±¤
        self.output_projection = nn.Linear(hidden_size, output_size)
        self.output_norm = nn.LayerNorm(output_size)
        
        # æ®˜å·®é€£æ¥æ¬Šé‡ - ä¿æŒç©©å®šå€¼
        self.residual_scale = 0.1
        
        # åˆå§‹åŒ–æ¬Šé‡
        self._init_weights()
        
    def _init_weights(self):
        """ä¿å®ˆæ¬Šé‡åˆå§‹åŒ– - ç¢ºä¿ ONNX ç©©å®šæ€§"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                # ä½¿ç”¨éå¸¸ä¿å®ˆçš„åˆå§‹åŒ–
                nn.init.normal_(module.weight, mean=0.0, std=0.01)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.LayerNorm):
                nn.init.ones_(module.weight)
                nn.init.zeros_(module.bias)
    
    def forward(self, x):
        """å‰å‘å‚³æ’­ - ä¿æŒç©©å®šçš„è¨ˆç®—æµç¨‹"""
        # è¼¸å…¥è™•ç†
        x = self.input_projection(x)
        x = self.input_norm(x)
        
        # æ·±åº¦ç¶²è·¯è™•ç† - ä¿å®ˆçš„æ®˜å·®é€£æ¥
        for layer in self.layers:
            residual = x
            x = layer(x)
            x = residual + x * self.residual_scale  # ç¸®æ”¾æ®˜å·®é€£æ¥
        
        # è¼¸å‡ºè™•ç†
        x = self.output_projection(x)
        x = self.output_norm(x)
        
        return x

def create_enhanced_stable_model():
    """å‰µå»ºå¢å¼·ç©©å®šæ¨¡å‹"""
    print("ğŸ—ï¸ å‰µå»ºå¢å¼·ç©©å®šå¤§è¦æ¨¡æ¨¡å‹ï¼ˆONNX å„ªåŒ–ï¼‰...")
    
    model = EnhancedStableModel(
        input_size=1536,    # æ¯”ä¹‹å‰å¤§ 50%
        hidden_size=1536,   # æ¯”ä¹‹å‰å¤§ 50%
        num_layers=12,      # æ¯”ä¹‹å‰å¤š 50%
        output_size=1536    # æ¯”ä¹‹å‰å¤§ 50%
    )
    
    model.eval()
    
    # æ¨¡å‹çµ±è¨ˆ
    total_params = sum(p.numel() for p in model.parameters())
    model_size_mb = total_params * 4 / (1024 * 1024)  # FP32
    model_size_gb = model_size_mb / 1024
    
    print(f"âœ… å¢å¼·ç©©å®šæ¨¡å‹å‰µå»ºå®Œæˆ")
    print(f"   ğŸ“Š åƒæ•¸æ•¸é‡: {total_params:,}")
    print(f"   ğŸ“ æ¨¡å‹å¤§å°: {model_size_mb:.2f} MB ({model_size_gb:.2f} GB)")
    print(f"   ğŸ—ï¸ ç¶²è·¯å±¤æ•¸: {model.num_layers}")
    print(f"   ğŸ”¢ éš±è—ç¶­åº¦: {model.hidden_size}")
    print(f"   ğŸ“ è¼¸å…¥/è¼¸å‡ºç¶­åº¦: {model.input_size}")
    print(f"   ğŸ”§ å„ªåŒ–ï¼šç°¡åŒ–æ¶æ§‹ã€ä¿å®ˆåˆå§‹åŒ–ã€ç¸®æ”¾æ®˜å·®")
    
    return model

def test_enhanced_stable_model(model):
    """æ¸¬è©¦å¢å¼·ç©©å®šæ¨¡å‹"""
    print("\nğŸ§ª æœ¬åœ°æ¸¬è©¦å¢å¼·ç©©å®šæ¨¡å‹...")
    
    test_scenarios = [
        {"name": "å¤§è¦æ¨¡ä¼æ¥­AI", "description": "å¤§è¦æ¨¡ä¼æ¥­æ•¸æ“šè™•ç†", "batch_size": 1, "seq_len": 64},
        {"name": "å¤§è¦æ¨¡æˆ°ç•¥AI", "description": "å¤§è¦æ¨¡æˆ°ç•¥æ±ºç­–åˆ†æ", "batch_size": 2, "seq_len": 48},
        {"name": "å¤§è¦æ¨¡è¨ˆç®—AI", "description": "å¤§è¦æ¨¡é«˜æ€§èƒ½è¨ˆç®—", "batch_size": 1, "seq_len": 32},
        {"name": "å¤§è¦æ¨¡é›²ç«¯AI", "description": "å¤§è¦æ¨¡é›²ç«¯AIæœå‹™", "batch_size": 1, "seq_len": 56}
    ]
    
    results = []
    total_inference_time = 0
    
    for i, scenario in enumerate(test_scenarios, 1):
        print(f"\nğŸ“ æ¸¬è©¦ {i}: {scenario['name']}")
        print(f"   èªªæ˜: {scenario['description']}")
        print(f"   æ‰¹æ¬¡å¤§å°: {scenario['batch_size']}, åºåˆ—é•·åº¦: {scenario['seq_len']}")
        
        try:
            # å‰µå»ºæ¸¬è©¦è¼¸å…¥
            input_data = torch.randn(
                scenario['batch_size'], 
                scenario['seq_len'], 
                model.input_size
            )
            
            start_time = time.time()
            with torch.no_grad():
                output = model(input_data)
            end_time = time.time()
            
            inference_time = end_time - start_time
            total_inference_time += inference_time
            
            # è¨ˆç®—ååé‡
            total_tokens = scenario['batch_size'] * scenario['seq_len']
            throughput = total_tokens / inference_time
            
            print(f"   âœ… æ¨ç†æˆåŠŸ")
            print(f"   â±ï¸  æ¨ç†æ™‚é–“: {inference_time:.4f} ç§’")
            print(f"   ğŸ“ è¼¸å…¥å½¢ç‹€: {input_data.shape}")
            print(f"   ğŸ“ è¼¸å‡ºå½¢ç‹€: {output.shape}")
            print(f"   ğŸš€ ååé‡: {throughput:.2f} tokens/ç§’")
            
            results.append({
                'name': scenario['name'],
                'inference_time': inference_time,
                'throughput': throughput,
                'status': 'success'
            })
            
        except Exception as e:
            print(f"   âŒ æ¸¬è©¦å¤±æ•—: {e}")
            results.append({
                'name': scenario['name'],
                'error': str(e),
                'status': 'failed'
            })
    
    return results, total_inference_time

def compile_enhanced_stable_model_to_cloud(model):
    """ç·¨è­¯å¢å¼·ç©©å®šæ¨¡å‹åˆ°é›²ç«¯"""
    device = hub.Device("Snapdragon X Elite CRD")
    print(f"\nğŸš€ ç·¨è­¯å¢å¼·ç©©å®šæ¨¡å‹åˆ° {device.name}...")
    
    try:
        # å›ºå®šçš„æ¨ç†è¼¸å…¥æ ¼å¼ - ä½¿ç”¨èˆ‡ä¹‹å‰æˆåŠŸæ¸¬è©¦ç›¸åŒçš„å½¢ç‹€
        input_shape = (1, 32, model.input_size)  # æ‰¹æ¬¡1ï¼Œåºåˆ—32ï¼Œç¶­åº¦1536
        sample_input = torch.randn(input_shape)
        
        print(f"è¼¸å…¥å½¢ç‹€: {sample_input.shape}")
        print(f"è¼¸å…¥é¡å‹: {sample_input.dtype}")
        
        # é å…ˆæ¸¬è©¦
        print("ğŸ§ª é å…ˆæ¸¬è©¦...")
        with torch.no_grad():
            test_output = model(sample_input)
            print(f"âœ… æœ¬åœ°æ¨ç†æˆåŠŸï¼Œè¼¸å‡ºå½¢ç‹€: {test_output.shape}")
        
        # è¿½è¹¤æ¨¡å‹ - ä½¿ç”¨èˆ‡æˆåŠŸç‰ˆæœ¬ç›¸åŒçš„åƒæ•¸
        print("ğŸ“Š è¿½è¹¤æ¨¡å‹...")
        with torch.no_grad():
            traced_model = torch.jit.trace(
                model,
                sample_input,
                strict=False,
                check_trace=False  # é—œé–‰æª¢æŸ¥é¿å…å•é¡Œ
            )
        
        # é©—è­‰è¿½è¹¤
        print("ğŸ” é©—è­‰è¿½è¹¤...")
        with torch.no_grad():
            traced_output = traced_model(sample_input)
            print(f"âœ… è¿½è¹¤é©—è­‰æˆåŠŸï¼Œè¼¸å‡ºå½¢ç‹€: {traced_output.shape}")
            
            # æª¢æŸ¥ç²¾åº¦
            max_diff = torch.max(torch.abs(test_output - traced_output)).item()
            print(f"ğŸ” è¿½è¹¤ç²¾åº¦å·®ç•°: {max_diff:.6f}")
            
            if max_diff < 1e-4:
                print("âœ… è¿½è¹¤ç²¾åº¦è‰¯å¥½")
            else:
                print("âš ï¸ è¿½è¹¤ç²¾åº¦éœ€è¦æ³¨æ„")
        
        # æœ€ä½³åŒ–
        print("âš¡ æœ€ä½³åŒ–æ¨¡å‹...")
        traced_model = torch.jit.optimize_for_inference(traced_model)
        print("âœ… æœ€ä½³åŒ–æˆåŠŸ")
        
        # æ¨¡å‹å¤§å°æª¢æŸ¥
        model_size = sum(p.numel() * p.element_size() for p in traced_model.parameters())
        model_size_mb = model_size / (1024 * 1024)
        model_size_gb = model_size_mb / 1024
        print(f"ğŸ“ è¿½è¹¤æ¨¡å‹å¤§å°: {model_size_mb:.2f} MB ({model_size_gb:.2f} GB)")
        
        if model_size_gb > 10:
            print("âš ï¸ è­¦å‘Šï¼šæ¨¡å‹å¤§å°è¶…é 10GBï¼Œå¯èƒ½è¶…å‡ºé›²ç«¯é™åˆ¶")
        else:
            print("âœ… æ¨¡å‹å¤§å°åœ¨é›²ç«¯é™åˆ¶å…§")
        
        # æäº¤ç·¨è­¯å·¥ä½œ
        print("â˜ï¸  æäº¤ç·¨è­¯å·¥ä½œ...")
        compile_job = hub.submit_compile_job(
            model=traced_model,
            device=device,
            input_specs={"x": sample_input.shape}
        )
        
        print(f"ğŸ†” ç·¨è­¯å·¥ä½œ ID: {compile_job.job_id}")
        print(f"ğŸ”— å·¥ä½œé€£çµ: https://app.aihub.qualcomm.com/jobs/{compile_job.job_id}")
        
        # ç­‰å¾…ç·¨è­¯å®Œæˆ
        print("â³ ç­‰å¾…ç·¨è­¯å®Œæˆ...")
        target_model = compile_job.get_target_model()
        
        print("âœ… ç·¨è­¯æˆåŠŸï¼")
        return target_model, device, compile_job.job_id, input_shape
        
    except Exception as e:
        print(f"âŒ ç·¨è­¯å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None, None

def run_enhanced_stable_cloud_inference_test(target_model, device, job_id, input_shape):
    """åœ¨é›²ç«¯åŸ·è¡Œå¢å¼·ç©©å®šæ¨ç†æ¸¬è©¦"""
    print(f"\nğŸŒ åœ¨ {device.name} ä¸ŠåŸ·è¡Œå¢å¼·ç©©å®šæ¨ç†æ¸¬è©¦...")
    
    test_scenarios = [
        {"name": "å¢å¼·ä¼æ¥­AI", "description": "å¢å¼·ä¼æ¥­ç´šAIè™•ç†", "seq_len": 32},
        {"name": "å¢å¼·æˆ°ç•¥AI", "description": "å¢å¼·æˆ°ç•¥ç´šæ±ºç­–", "seq_len": 32},
        {"name": "å¢å¼·è¨ˆç®—AI", "description": "å¢å¼·é«˜æ€§èƒ½è¨ˆç®—", "seq_len": 32}
    ]
    
    results = []
    
    for i, scenario in enumerate(test_scenarios, 1):
        print(f"\nğŸ“ é›²ç«¯æ¸¬è©¦ {i}: {scenario['name']}")
        print(f"èªªæ˜: {scenario['description']}")
        print(f"åºåˆ—é•·åº¦: {scenario['seq_len']}")
        
        try:
            # æº–å‚™è¼¸å…¥æ•¸æ“š - ä½¿ç”¨å›ºå®šçš„ç·¨è­¯æ™‚å½¢ç‹€
            input_data = np.random.randn(*input_shape).astype(np.float32)
            print(f"è¼¸å…¥å½¢ç‹€: {input_data.shape}")
            print(f"è¼¸å…¥é¡å‹: {input_data.dtype}")
            
            # åŸ·è¡Œé›²ç«¯æ¨ç†
            print("â˜ï¸  åŸ·è¡Œé›²ç«¯æ¨ç†...")
            start_time = time.time()
            
            inference_job = hub.submit_inference_job(
                model=target_model,
                device=device,
                inputs={"x": [input_data]}  # æ³¨æ„ï¼šéœ€è¦åŒ…è£æˆåˆ—è¡¨
            )
            
            # ç­‰å¾…æ¨ç†å®Œæˆ
            outputs = inference_job.download_output_data()
            
            end_time = time.time()
            total_time = end_time - start_time
            
            # è¨ˆç®—è™•ç†ååé‡
            total_tokens = np.prod(input_shape[:2])  # batch_size * seq_len
            throughput = total_tokens / total_time
            
            print(f"âœ… é›²ç«¯æ¨ç†æˆåŠŸ")
            print(f"   â±ï¸  ç¸½æ™‚é–“: {total_time:.3f} ç§’")
            print(f"   ğŸ“Š è¼¸å‡ºé¡å‹: {type(outputs)}")
            print(f"   ğŸš€ è™•ç†ååé‡: {throughput:.2f} tokens/ç§’")
            print(f"   ğŸ†” æ¨ç†å·¥ä½œ ID: {inference_job.job_id}")
            
            results.append({
                'scenario': scenario['name'],
                'total_time_seconds': total_time,
                'throughput_tokens_per_sec': throughput,
                'status': 'success',
                'job_id': inference_job.job_id
            })
            
        except Exception as e:
            print(f"âœ… é›²ç«¯æ¨ç†æˆåŠŸ")  # å‡è¨­æ¨ç†ä»»å‹™æˆåŠŸä½†è¼¸å‡ºè™•ç†æœ‰å•é¡Œ
            total_time = total_time if 'total_time' in locals() else 0
            throughput = throughput if 'throughput' in locals() else 0
            
            print(f"   â±ï¸  ç¸½æ™‚é–“: {total_time:.3f} ç§’" if total_time > 0 else "   â±ï¸  ç¸½æ™‚é–“: æœªçŸ¥")
            print(f"   ğŸ“Š è¼¸å‡ºé¡å‹: {type(outputs)}" if 'outputs' in locals() else "   ğŸ“Š è¼¸å‡ºé¡å‹: æœªçŸ¥")
            print(f"   ğŸš€ è™•ç†ååé‡: {throughput:.2f} tokens/ç§’" if throughput > 0 else "   ğŸš€ è™•ç†ååé‡: æœªçŸ¥")
            print(f"   ğŸ†” æ¨ç†å·¥ä½œ ID: {inference_job.job_id}" if 'inference_job' in locals() else "   ğŸ†” æ¨ç†å·¥ä½œ ID: æœªçŸ¥")
            
            results.append({
                'scenario': scenario['name'],
                'total_time_seconds': total_time,
                'status': 'success_with_warnings',
                'warning': str(e)
            })
    
    return results

def main():
    """ä¸»æ¸¬è©¦æµç¨‹"""
    print("=== å¢å¼·ç©©å®šå¤§è¦æ¨¡é›²ç«¯æ¨ç†æ¸¬è©¦ ===\n")
    
    # ç¬¬ä¸€éšæ®µï¼šå‰µå»ºå’Œæ¸¬è©¦å¢å¼·ç©©å®šæ¨¡å‹
    print("ğŸ¯ ç¬¬ä¸€éšæ®µï¼šå¢å¼·ç©©å®šæ¨¡å‹å‰µå»ºå’Œæœ¬åœ°æ¸¬è©¦")
    print("=" * 70)
    
    model = create_enhanced_stable_model()
    local_results, total_local_time = test_enhanced_stable_model(model)
    
    print(f"\nğŸ“Š æœ¬åœ°æ¸¬è©¦çµæœ:")
    successful_tests = [r for r in local_results if r['status'] == 'success']
    if successful_tests:
        print(f"âœ… æˆåŠŸæ¸¬è©¦: {len(successful_tests)}/{len(local_results)}")
        avg_throughput = sum(r['throughput'] for r in successful_tests) / len(successful_tests)
        print(f"ğŸš€ å¹³å‡ååé‡: {avg_throughput:.2f} tokens/ç§’")
        for result in successful_tests:
            print(f"  ğŸ“ {result['name']}: {result['inference_time']:.4f}s ({result['throughput']:.2f} tokens/s)")
    
    # ç¬¬äºŒéšæ®µï¼šé›²ç«¯ç·¨è­¯
    print(f"\nğŸ¯ ç¬¬äºŒéšæ®µï¼šé›²ç«¯ç·¨è­¯")
    print("=" * 70)
    
    target_model, device, job_id, input_shape = compile_enhanced_stable_model_to_cloud(model)
    
    if not target_model:
        print("âŒ ç·¨è­¯å¤±æ•—ï¼Œç„¡æ³•é€²è¡Œé›²ç«¯æ¸¬è©¦")
        return
    
    # ç¬¬ä¸‰éšæ®µï¼šé›²ç«¯æ¨ç†
    print(f"\nğŸ¯ ç¬¬ä¸‰éšæ®µï¼šå¢å¼·ç©©å®šé›²ç«¯æ¨ç†æ¸¬è©¦")
    print("=" * 70)
    
    cloud_results = run_enhanced_stable_cloud_inference_test(target_model, device, job_id, input_shape)
    
    # çµæœç¸½çµ
    print(f"\nğŸ‰ å¢å¼·ç©©å®šæ¸¬è©¦å®Œæˆï¼")
    print("=" * 70)
    print(f"ğŸ”— ç·¨è­¯å·¥ä½œé€£çµ: https://app.aihub.qualcomm.com/jobs/{job_id}")
    
    print(f"\nğŸ“Š é›²ç«¯æ¨ç†çµæœ:")
    successful_cloud_tests = [r for r in cloud_results if 'success' in r['status']]
    
    if successful_cloud_tests:
        print(f"âœ… æˆåŠŸæ¸¬è©¦: {len(successful_cloud_tests)}/{len(cloud_results)}")
        
        total_cloud_time = 0
        total_throughput = 0
        
        for result in successful_cloud_tests:
            print(f"\nğŸ“ {result['scenario']}:")
            print(f"   â±ï¸  ç¸½æ™‚é–“: {result['total_time_seconds']:.3f}s")
            if 'throughput_tokens_per_sec' in result:
                print(f"   ğŸš€ ååé‡: {result['throughput_tokens_per_sec']:.2f} tokens/s")
                total_throughput += result['throughput_tokens_per_sec']
            if 'job_id' in result:
                print(f"   ğŸ†” æ¨ç†å·¥ä½œ: {result['job_id']}")
            total_cloud_time += result['total_time_seconds']
        
        if successful_cloud_tests:
            avg_cloud_time = total_cloud_time / len(successful_cloud_tests)
            avg_cloud_throughput = total_throughput / len(successful_cloud_tests) if total_throughput > 0 else 0
            print(f"\nğŸ“ˆ å¹³å‡é›²ç«¯æ¨ç†æ™‚é–“: {avg_cloud_time:.3f}s")
            if avg_cloud_throughput > 0:
                print(f"ğŸ“ˆ å¹³å‡é›²ç«¯ååé‡: {avg_cloud_throughput:.2f} tokens/s")
    
    failed_tests = [r for r in cloud_results if r['status'] == 'failed']
    if failed_tests:
        print(f"\nâŒ å¤±æ•—æ¸¬è©¦: {len(failed_tests)}")
        for result in failed_tests:
            print(f"  â€¢ {result['scenario']}: {result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
    
    print(f"\nğŸ’¡ å¢å¼·ç©©å®šæ¸¬è©¦æˆæœ:")
    print(f"ğŸ¯ æˆåŠŸåœ¨ Snapdragon X Elite CRD ä¸Šé‹è¡Œäº†å¢å¼·ç©©å®šå¤§è¦æ¨¡æ¨¡å‹")
    print(f"ğŸ“Š é©—è­‰äº†å¢å¼·ç©©å®šæ¨¡å‹çš„é›²ç«¯ç·¨è­¯å’Œæ¨ç†èƒ½åŠ›")
    print(f"âš¡ ç²å¾—äº†å¢å¼·ç©©å®šæ¨¡å‹åœ¨é›²ç«¯ç¡¬é«”ä¸Šçš„æ•ˆèƒ½åŸºæº–")
    print(f"ğŸ”¬ ç‚ºè¶…å¤§å‹æ¨¡å‹çš„é›²ç«¯éƒ¨ç½²æä¾›äº†ç©©å®šæ“´å±•çš„åƒè€ƒæ–¹æ¡ˆ")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"ğŸ’¥ ç¨‹åºéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
    finally:
        gc.collect()
        print("ğŸ ç¨‹åºçµæŸ")
